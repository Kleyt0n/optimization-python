{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from optymus import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.Z1 = jnp.dot(X, self.W1) + self.b1\n",
    "        self.A1 = jnp.tanh(self.Z1)\n",
    "        self.Z2 = jnp.dot(self.A1, self.W2) + self.b2\n",
    "        return self.Z2\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        # Mean Squared Error Loss\n",
    "        y_pred = self.forward(X)\n",
    "        return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "    def get_params(self):\n",
    "        # Get network parameters\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # Set network parameters\n",
    "        self.W1, self.b1, self.W2, self.b2 = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(X_train, y_train, nn, learning_rate=0.01, max_iter=100, tol=1e-4, verbose=True, optim='gradient_descent'):\n",
    "    # Flatten the parameters to optimize\n",
    "    params = nn.get_params()\n",
    "    flat_params = np.concatenate([p.flatten() for p in params])\n",
    "\n",
    "    def f_obj(flat_params):\n",
    "        # Unflatten the parameters\n",
    "        shapes = [p.shape for p in params]\n",
    "        sizes = [np.prod(shape) for shape in shapes]\n",
    "        new_params = []\n",
    "        index = 0\n",
    "        for size, shape in zip(sizes, shapes):\n",
    "            new_params.append(flat_params[index:index + size].reshape(shape))\n",
    "            index += size\n",
    "        nn.set_params(new_params)\n",
    "        return nn.loss(X_train, y_train)\n",
    "    \n",
    "    opt = Optimizer(f_obj=f_obj, x0=flat_params, learning_rate=learning_rate, max_iter=max_iter, tol=tol, verbose=verbose, method=optim)\n",
    "    result = opt.get_results()\n",
    "    \n",
    "    # Set the optimal parameters back to the network\n",
    "    shapes = [p.shape for p in params]\n",
    "    sizes = [np.prod(shape) for shape in shapes]\n",
    "    params = []\n",
    "    index = 0\n",
    "    for size, shape in zip(sizes, shapes):\n",
    "        params.append(result['xopt'][index:index + size].reshape(shape))\n",
    "        index += size\n",
    "    nn.set_params(params)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam 0:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam 0: 100%|██████████| 100/100 [00:01<00:00, 84.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: [ 0.01977234 -0.00582409  0.04228204  0.011235   -0.01882133  0.02333755\n",
      "  0.00631271  0.02514589 -0.00213405 -0.01806712 -0.04017076  0.00631022\n",
      " -0.05040128 -0.00588307  0.0253066  -0.1451187  -0.18447872 -0.00876499\n",
      " -0.17868824 -0.19668901  0.09780558  0.12132452  0.00746448  0.11234963\n",
      "  0.13512725 -0.2021622 ]\n",
      "Minimum loss: 8.341569418836764\n",
      "Number of iterations: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X_train = np.random.randn(100, 3)\n",
    "y_train = np.dot(X_train, np.array([1.5, -2.0, 1.0])) + 0.5 * np.random.randn(100)\n",
    "\n",
    "# Define the neural network\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "nn = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "result = train_neural_network(X_train, y_train, nn, learning_rate=0.01, max_iter=100, tol=1e-4, verbose=True, optim='gradient_descent')\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimal parameters:\", result['xopt'])\n",
    "print(\"Minimum loss:\", result['fmin'])\n",
    "print(\"Number of iterations:\", result['num_iter'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optymus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
